{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# a_tensor_initialization.py",
   "id": "555080b6f0fb1217"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:33.196365Z",
     "start_time": "2025-09-27T11:25:33.054872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 기본 데이터 타입 float32\n",
    "t1 = torch.Tensor([1,2,3], device='cpu')\n",
    "print(t1.dtype)\n",
    "print(t1.device)\n",
    "print(t1.requires_grad)\n",
    "print(t1.size())\n",
    "print(t1.shape)\n",
    "\n",
    "t2 = torch.tensor([1,2,3], device='cpu')\n",
    "print(t2.dtype)\n",
    "print(t2.device)\n",
    "print(t2.requires_grad)\n",
    "print(t2.size())\n",
    "print(t2.shape)\n",
    "\n",
    "a1 = torch.tensor(1)\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "a9 = torch.tensor([\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]],\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "# 내부 리스트들의 길이가 다르면 에러가 발생함\n",
    "a11 = torch.tensor([\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[96], line 82\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mprint\u001B[39m(a10\u001B[38;5;241m.\u001B[39mshape, a10\u001B[38;5;241m.\u001B[39mndim)\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# 내부 리스트들의 길이가 다르면 에러가 발생함\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m a11 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "위 코드 블럭은 torch.Tensor와 torch.tensor의 차이점을 시각적으로 보여주고 만들어진 tensor의 shape와 ndim을 계속 보여줌으로 tensor에 대한 이해를 도와주고 있다고 생각합니다.\n",
    "마지막 부분의 에러가 나는 부분을 보면 한쪽은 [1, 2, 3], 다른 한쪽은 [4, 5] 형태로 길이가 서로 다름을 알 수 있습니다. 그렇기에 shape 불일치 오류가 발생하며 다차원 텐서를 생성할 때, 내부 리스트들의 크기가 일치해야함을 시각적으로 알 수 있었습니다.\n"
   ],
   "id": "4df17fc7ba6da691"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# b_tensor_initialization_copy.py",
   "id": "e1a2cd08c06bb983"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:42.694402Z",
     "start_time": "2025-09-27T11:25:42.683553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)  # 데이터를 복사해 새로운 텐서 생성\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)  # 복사본은 영향을 안받음\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch. Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100  # 원본 numpy 배열을 변경하면 텐서도 함께 변경\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ],
   "id": "e57c8c07367b0e4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드 블록은 텐서 생성 함수들이 원본 데이터의 값을 복사하는지, 아니면 메모리 주소를 공유하는지를 비교하여 보여주고 있습니다. 특히 NumPy 배열을 torch.as_tensor로 변환한 경우, 원본 배열의 값을 변경하자 텐서의 값도 함께 바뀌는 것을 확인할 수 있었습니다."
   ],
   "id": "ad446e5162c90765"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# c_tensor_initialization_constant_values.py",
   "id": "206ca4fb555e6ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:42.740677Z",
     "start_time": "2025-09-27T11:25:42.725368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))\n",
    "t1_like = torch.ones_like(input=t1)  # 입력과 동일한 shape, dtype을 갖는 텐서 생성\n",
    "print(t1)\n",
    "print(t1_like)\n",
    "\n",
    "t2 = torch.zeros(size=(6,))\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)\n",
    "print(t2_like)\n",
    "\n",
    "t3 = torch.empty(size=(4,))\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)\n",
    "print(t3_like)\n",
    "\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)"
   ],
   "id": "2c8277d8baa521a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([  1.0000,   1.8750,  27.0000, 256.0000])\n",
      "tensor([0.0000, 2.1250, 3.0000, 5.0000])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "해당 코드는 ones, zeros, empty, eye와 같이 특정 상수 값으로 채워진 텐서를 생성하는 방법을 보여줍니다. _like 함수를 사용하면 기존 텐서의 shape이나 dtype 같은 속성을 그대로 가져와 새로운 텐서를 만들 수 있어 편리하다는 것을 알 수 있었습니다. 또한 empty는 텐서를 초기화하지 않고 메모리 공간만 할당하기 때문에, 예측할 수 없는 값이 들어있다는 점을 눈으로 직접 확인할 수 있었습니다."
   ],
   "id": "746a0ddc4dae4e3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# d_tensor_initialization_random_values.py",
   "id": "25f67b0150fb17ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:42.802865Z",
     "start_time": "2025-09-27T11:25:42.776346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "# 평균 0, 표준편차 1의 표준 정규 분포에서 값을 샘플링\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)\n",
    "\n",
    "# 난수 생성 시드를 고정\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729) # 위와 값은 값으로 고정했기에 같은 텐서 생성됨\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ],
   "id": "a9036f1cf781d090",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 14]])\n",
      "tensor([[0.2985, 0.2210, 0.1837]])\n",
      "tensor([[-2.1456,  0.3064, -0.4560]])\n",
      "tensor([[12.1297,  9.4018],\n",
      "        [10.4189,  9.1229],\n",
      "        [ 9.7090,  9.6984]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드 블록은 다양한 종류의 무작위 텐서를 생성하는 방법을 다루고 있습니다. randint, rand, randn 등 각 함수가 어떤 확률 분포를 따르는지 보여주고 있습니다.\n",
    "torch.manual_seed는 시드 값을 고정하면 난수 생성이 동일하게 반복되는 것을 random1과 random3의 비교를 통해서 확인할 수 있었습니다."
   ],
   "id": "ec133e384cccb010"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# e_tensor_type_conversion.py",
   "id": "976d7421db348662"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:42.882179Z",
     "start_time": "2025-09-27T11:25:42.853301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2).type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)"
   ],
   "id": "c6ad83d68656f594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "위 코드는 텐서를 생성할 때 데이터 타입을 지정하거나, 이미 생성된 텐서의 타입을 변환하는  다양한 방법을 보여주고 있습니다. 여러 메소드가 있지만 적절히 사용하면 전부 원하는 데이터 타입으로 변경할 수 있음을 깨달았습니다. 마지막에 서로 다른 타입의 텐서들을 연산했을 때, 자동으로 torch.float64로 결과가 나오는 것을 보고 Pytorch의 타입 처리 방식에 대해서 더 이해할 수 있었던 것 같습니다."
   ],
   "id": "6fc20b8d3d2a0d4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# f_tensor_operations.py",
   "id": "b8d3d2f871a27a9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:42.944694Z",
     "start_time": "2025-09-27T11:25:42.931699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ],
   "id": "c586deea3675e30a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "위 코드는 텐서의 기본적인 사칙연산을 보여주고 있습니다. torch.add와 같은 함수를 사용하는 방법과 + 같은 연산자를 사용하는 방법, 두 가지 모두 동일한 결과를 보여준다는 것을 시각적으로 알 수 있었습니다."
   ],
   "id": "5b24bb188f901760"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# g_tensor_operations_mm.py",
   "id": "6f994703d5cb4fe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.037452Z",
     "start_time": "2025-09-27T11:25:43.024558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 내적 계산\n",
    "t1 = torch.dot(\n",
    "    torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3) # 2차원 텐서간의 행렬 곱셈 계산\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6) # 3차원 배치 행렬 곱셈\n",
    "print(t7.size())"
   ],
   "id": "cc84c643ddce5359",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드는 다양한 행렬 곱셈 연산의 차이점을 보여주고 있습니다. 각각의 연산들을 직접 사용 해보고 값을 확인 해보며 이해에 더 도움이 되었습니다."
   ],
   "id": "87898ae19dd6ed58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# h_tensor_operations_matmul.py",
   "id": "38e392776614a9ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.067910Z",
     "start_time": "2025-09-27T11:25:43.054317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size()) # 1D, 1D 이기에 내적\n",
    "\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())\n",
    "\n",
    "t7 = torch.rand(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size()) # 3D, 3D 이기에 배치 행렬 곱셈\n",
    "\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())"
   ],
   "id": "8b9bf7f42239ebec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드 블록은 torch.matmul이라는 함수가 텐서의 shape에 따라 어떻게 동작하는지 보여주고 있습니다. 입력에 따라서 알아서 g 파트에 있던 dot, mm, bmm을 사용하고 있기에 매우 편리한 기능이라고 생각했습니다."
   ],
   "id": "15c9fa33f0ac56ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# i_tensor_broadcasting.py",
   "id": "9d30de1b0c2c4eae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.145768Z",
     "start_time": "2025-09-27T11:25:43.116795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2) # shape가 서로 달라서 작은 쪽이 확장됨\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4) # 이 경우 t4가 더 작아서 [[4, 5], [4, 5], [4, 5]]로 확장됨\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)\n",
    "print(t5 - 2.0)\n",
    "print(t5 * 2.0)\n",
    "print(t5 / 2.0)\n",
    "\n",
    "def normalize(x):\n",
    "    return x / 255\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])\n",
    "t8 = torch.tensor([[3, 1]])\n",
    "t9 = torch.tensor([[5], [2]])\n",
    "t10 = torch.tensor([7])\n",
    "print(t7 + t8)\n",
    "print(t7 + t9)\n",
    "print(t8 + t9)\n",
    "print(t7 + t10)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)\n",
    "print((t17 + t18).size())\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)\n",
    "\n",
    "exp = torch.arange(1., 5.)\n",
    "a = torch.arange(1., 5.)\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)"
   ],
   "id": "3a1f1330ddfd64cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "torch.Size([3, 28, 28])\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "위 코드는 shape가 서로 다른 텐서끼리 연산을 다양한 예시를 통해서 설명해주고 있습니다. 크기가 작은 텐서가 크기가 큰 텐서의 shape에 맞춰 확장되어 연산되는 과정을 다양한 shape 조합들로 보여주어 원리를 쉽게 이해할 수 있었습니다."
   ],
   "id": "b607a566bb54ba33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# j_tensor_indexing_slicing.py",
   "id": "1f96c5db0ce07a7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.223416Z",
     "start_time": "2025-09-27T11:25:43.195247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "    [[0, 1, 2, 3, 4],\n",
    "     [5, 6, 7, 8, 9],\n",
    "     [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])\n",
    "print(x[:, 1])  # ':'는 해당 차원의 모든 요소를 의미\n",
    "print(x[1, 2])\n",
    "print(x[:, -1])\n",
    "\n",
    "print(x[1:])\n",
    "print(x[1:, 3:]) # 이 경우는 1번 행부터 끝까지, 3번 열부터 끝까지\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "z = torch.tensor(\n",
    "    [[1, 2, 3, 4],\n",
    "     [2, 3, 4, 5],\n",
    "     [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "# 슬라이싱으로 선택된 영역에 값을 할당하면 원본 텐서가 변경됨\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ],
   "id": "5be50087be27f9a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드 블록은 텐서의 특정 원소나 부분에 접근하는 인덱싱과 슬라이싱 기법을 보여주고 있습니다. 행, 열, 특정 구간을 자유롭게 잘라내고, 심지어 해당 부분의 값을 한 번에 변경할 수 있다는 것을 y[1:4, 2] = 1 같은 코드를 통해 확인할 수 있었습니다. 텐서 데이터를 다룰 때 원하는 부분을 효과적으로 제어하는 필수적인 방법임을 알게 되었습니다."
   ],
   "id": "7800e57bcdec0d06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# k_tensor_reshaping.py",
   "id": "74e8d273b9881477"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.300656Z",
     "start_time": "2025-09-27T11:25:43.272733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2) # view()는 원소 개수를 유지하며 shape 변경\n",
    "t3 = t1.reshape(1, 6) # reshape()은 view()와 유사하지만, 메모리가 비연속적이여도 가능함\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "# squeeze()는 크기가 1인 차원을 제거하여 차원을 축소함\n",
    "t7 = t6.squeeze()\n",
    "\n",
    "\n",
    "t8 = t6.squeeze()\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "# unsqueeze()는 지정된 dim에 크기가 1인 차원을 추가함\n",
    "t10 = t9.unsqueeze(1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "t14 = t13.flatten()\n",
    "\n",
    "print(t14)\n",
    "\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)\n",
    "print(torch.permute(t18, (2, 0, 1)).size())\n",
    "# permute()는 괄호 안의 순서대로 차원을 재배치함\n",
    "\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "t20 = torch.permute(t19, dims=(0, 1))\n",
    "t21 = torch.permute(t19, dims=(1, 0))\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "t22 = torch.transpose(t19, 0, 1)\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)\n",
    "print(t23)"
   ],
   "id": "3ea4ca6ff45aa4c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드는 텐서의 전체 원소 수는 유지하면서 shape을 바꾸는 다양한 방법을 보여주고 있습니다. view와 reshape의 기본적인 기능부터, 크기가 1인 차원을 제거하는 squeeze, 추가하는 unsqueeze 방법을 시각적인 결과로 볼 수 있어서 이해하기 좋았습니다. 그리고 permute의 경우 차원의 순서를 원하는 대로 재배치 할 수 있기에 이미지나 여러 부분에서 유용하겠다고 생각했습니다."
   ],
   "id": "ba246ba32319bdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# l_tensor_concat.py",
   "id": "faae4e1a05646073"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.378759Z",
     "start_time": "2025-09-27T11:25:43.350290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "# torch.cat()은 기존의 dim을 따라 여러 텐서를 이어 붙여줌\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)\n",
    "\n",
    "t5 = torch.arange(0, 3)\n",
    "t6 = torch.arange(3, 8)\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)\n",
    "print(t7)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())\n",
    "print(t16)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())\n",
    "print(t19)\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())\n",
    "print(t20)\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())\n",
    "print(t21)"
   ],
   "id": "fb3ad7e482916ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드 블록은 torch.cat 함수를 사용하여 여러 텐서를 특정 차원을 기준으로 이어 붙이는 것을 보여주고 있습니다. dim의 값에 따라서 어떻게 달라지는지 출력 값을 통해서 확인할 수 있었습니다."
   ],
   "id": "8d2481d8be6e36e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# m_tensor_stacking.py",
   "id": "7466f3418713dd80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.455563Z",
     "start_time": "2025-09-27T11:25:43.441143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# torch.stack()은 새로운 dim을 만들어 텐서들을 쌓음\n",
    "# 입력 텐서들의 shape이 모두 동일해야 함\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "# stack(dim=0)은 각 텐서에 unsqueeze(dim=0)을 적용한 뒤 cat(cat=0)하는 것과 동일함\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "t9 = torch.arange(0, 3)\n",
    "t10 = torch.arange(3, 6)\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())\n",
    "print(t13)\n",
    "\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))"
   ],
   "id": "36a5bfb4d3876cd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "이 코드는 torch.stack이 torch.cat에 대해서 비교하며 자세히 보여주고 있습니다. 과정과 결과를 보면 stack은 새로운 차원을 만들어 그곳에 텐서를 쌓고 cat은 기존 차원을 따라서 합친다는 것을 알 수 있었습니다. 마지막으로, unsqueeze 연산을 잘 활용하면 torch.cat을 통홰 torch.stack과 동일한 결과를 얻을 수 있음 또한 알 수 있었습니다."
   ],
   "id": "ee81754aa1b50173"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# n_tensor_vstack_hstack.py",
   "id": "a0b1da8b5911286c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T11:25:43.503073Z",
     "start_time": "2025-09-27T11:25:43.481606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))  # vstack은 수직으로 쌓음\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "\n",
    "t7 = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "    [[13, 14, 15], [16, 17, 18]],\n",
    "    [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "\n",
    "print(t9)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11)) # hstack은 수평으로 쌓음\n",
    "print(t12)\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "    [[13, 14, 15], [16, 17, 18]],\n",
    "    [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "\n",
    "print(t18)"
   ],
   "id": "16e69bb546099cc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 기술적 사항 / 고찰 내용\n",
    "마지막 코드 블록은 텐서를 수직또는 수평으로 쌓는 방법을 직관적으로 보여주고 있습니다. 특히 cat 함수에서 dim을 지정하는 대신, vstack이나 hstack을 사용해서 코드가 더 직관적으로 보인다고 생각했습니다. 마지막으로, 결과 출력을 통해서 실제로 어떻게 되었는지 볼 수 있어서 메소드의 이해에 도움이 되었습니다."
   ],
   "id": "f53b98e08a45497d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 숙제후기\n",
    "이번 과제를 통해서 tensor가 무엇이고 tensor를 이용하는 다양한 방법들에 대해서 실습해 볼 수 있었습니다. 양이 많았던 만큼 전부 기억에는 힘들어도 이번 과제가 경험이 되어서 사용함에 있어서는 문제 없을 것이라고 생각이 들었습니다.\n",
    "처음에는 많은 양에 힘들었지만,딥러닝에 관해서 스스로 공부를 더 해볼 수 있게 만들어주는 과제였다고 생각합니다."
   ],
   "id": "754a04d63dd76a42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
